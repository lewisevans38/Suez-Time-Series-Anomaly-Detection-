{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOnzgWOWZmz0U7635oU/62y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lewisevans38/Suez-Time-Series-Anomaly-Detection-/blob/main/mnist_gan_modecollapse.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code to induce a mode collapse in the MNIST GAN. The code to make the GAN work is the same but with nz= 256\n",
        "\n",
        "The code follows the tutorial from https://debuggercafe.com/vanilla-gan-pytorch/."
      ],
      "metadata": {
        "id": "yewTRrU7MRb8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LfC48Ntz_sb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import torchvision.datasets as datasets\n",
        "import imageio\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "\n",
        "from torchvision.utils import make_grid, save_image\n",
        "from torch.utils.data import DataLoader\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# learning parameters\n",
        "batch_size = 512\n",
        "epochs = 100\n",
        "sample_size = 64 # fixed sample size\n",
        "nz = 32 # latent vector size\n",
        "k = 1 # number of steps to apply to the discriminator\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ],
      "metadata": {
        "id": "7g9rA9En0Foy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,),(0.5,)),\n",
        "])\n",
        "to_pil_image = transforms.ToPILImage()"
      ],
      "metadata": {
        "id": "VSjivQU10Oo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = datasets.MNIST(\n",
        "    root='../input/data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "46XDHAkl0QwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First create a simple MLP generator, with leaky relu inbetween linear layers."
      ],
      "metadata": {
        "id": "ieZMnO-508uS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, nz):\n",
        "        super(Generator, self).__init__()\n",
        "        self.nz = nz\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Linear(self.nz, 256),\n",
        "            nn.LeakyReLU(0.2), ##LeakyRELU is a function that is used to help mitigate the VGP\n",
        "            nn.Linear(256, 512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(1024, 784),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.main(x).view(-1, 1, 28, 28)"
      ],
      "metadata": {
        "id": "sPWltr8u0cY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Discriminator is a binary classifier -- \"Is the data real or fake?\""
      ],
      "metadata": {
        "id": "dlbwB9u81Pei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Discriminator,self).__init__()\n",
        "    self.n_input = 784 #flattened size of MNIST\n",
        "    self.main = nn.Sequential(\n",
        "        nn.Linear(self.n_input, 1024),\n",
        "        nn.LeakyReLU(0.2),\n",
        "        nn.Dropout(0.3),\n",
        "        nn.Linear(1024, 512),\n",
        "        nn.LeakyReLU(0.2),\n",
        "        nn.Dropout(0.3),\n",
        "        nn.Linear(512, 256),\n",
        "        nn.LeakyReLU(0.2),\n",
        "        nn.Dropout(0.3),\n",
        "        nn.Linear(256, 1),\n",
        "        nn.Sigmoid(),\n",
        "    )\n",
        "  def forward(self, x):\n",
        "      x = x.view(-1, 784)\n",
        "      return self.main(x)"
      ],
      "metadata": {
        "id": "Va-fS9AH0zRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = Generator(nz).to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "print('##### GENERATOR #####')\n",
        "print(generator)\n",
        "print('######################')\n",
        "print('\\n##### DISCRIMINATOR #####')\n",
        "print(discriminator)\n",
        "print('######################')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1YoZP322K9r",
        "outputId": "4ff40897-065c-42f2-d531-77be44e18554"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##### GENERATOR #####\n",
            "Generator(\n",
            "  (main): Sequential(\n",
            "    (0): Linear(in_features=32, out_features=256, bias=True)\n",
            "    (1): LeakyReLU(negative_slope=0.2)\n",
            "    (2): Linear(in_features=256, out_features=512, bias=True)\n",
            "    (3): LeakyReLU(negative_slope=0.2)\n",
            "    (4): Linear(in_features=512, out_features=1024, bias=True)\n",
            "    (5): LeakyReLU(negative_slope=0.2)\n",
            "    (6): Linear(in_features=1024, out_features=784, bias=True)\n",
            "    (7): Tanh()\n",
            "  )\n",
            ")\n",
            "######################\n",
            "\n",
            "##### DISCRIMINATOR #####\n",
            "Discriminator(\n",
            "  (main): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=1024, bias=True)\n",
            "    (1): LeakyReLU(negative_slope=0.2)\n",
            "    (2): Dropout(p=0.3, inplace=False)\n",
            "    (3): Linear(in_features=1024, out_features=512, bias=True)\n",
            "    (4): LeakyReLU(negative_slope=0.2)\n",
            "    (5): Dropout(p=0.3, inplace=False)\n",
            "    (6): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (7): LeakyReLU(negative_slope=0.2)\n",
            "    (8): Dropout(p=0.3, inplace=False)\n",
            "    (9): Linear(in_features=256, out_features=1, bias=True)\n",
            "    (10): Sigmoid()\n",
            "  )\n",
            ")\n",
            "######################\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizers\n",
        "optim_g = optim.Adam(generator.parameters(), lr=0.0002)\n",
        "optim_d = optim.Adam(discriminator.parameters(), lr=0.0002)\n",
        "\n",
        "# loss function\n",
        "criterion = nn.BCELoss() ##Binary Cross entropy loss, A normal loos function for binary classification tasks.\n",
        "\n",
        "losses_g = [] # to store generator loss after each epoch\n",
        "losses_d = [] # to store discriminator loss after each epoch\n",
        "images = [] # to store images generatd by the generator"
      ],
      "metadata": {
        "id": "sbupmogt2Pxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper functions to create labels"
      ],
      "metadata": {
        "id": "9DaqXa303LDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to create real labels (1s)\n",
        "def label_real(size):\n",
        "    data = torch.ones(size, 1)\n",
        "    return data.to(device)\n",
        "# to create fake labels (0s)\n",
        "def label_fake(size):\n",
        "    data = torch.zeros(size, 1)\n",
        "    return data.to(device)"
      ],
      "metadata": {
        "id": "0Iudv67z3KnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For generating fake images, we need to provide the generator with a noise vector. The size of the noise vector should be equal to nz that we have defined earlier. To create this noise vector, we define the function below"
      ],
      "metadata": {
        "id": "EBB9I8rm2s5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to create the noise vector\n",
        "def create_noise(sample_size, nz):\n",
        "    return torch.randn(sample_size, nz).to(device)\n",
        "\n",
        "##Also need to save the images, which we use the following function for\n",
        "\n",
        "def save_generator_image(image, path):\n",
        "    save_image(image, path)"
      ],
      "metadata": {
        "id": "xnVJrFpP2j-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Discriminator:"
      ],
      "metadata": {
        "id": "mFsVS1TF3Bwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to train the discriminator network\n",
        "def train_discriminator(optimizer, data_real, data_fake):\n",
        "    b_size = data_real.size(0)\n",
        "    real_label = label_real(b_size)\n",
        "    fake_label = label_fake(b_size)\n",
        "    optimizer.zero_grad()\n",
        "    output_real = discriminator(data_real)\n",
        "    loss_real = criterion(output_real, real_label)\n",
        "    output_fake = discriminator(data_fake)\n",
        "    loss_fake = criterion(output_fake, fake_label)\n",
        "    loss_real.backward()\n",
        "    loss_fake.backward()\n",
        "    optimizer.step()\n",
        "    return loss_real + loss_fake"
      ],
      "metadata": {
        "id": "d_zIUV_c3bfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to train the Generator Network\n",
        "def train_generator(optimizer, data_fake):\n",
        "    b_size = data_fake.size(0)\n",
        "    real_label = label_real(b_size)\n",
        "    optimizer.zero_grad()\n",
        "    output = discriminator(data_fake)\n",
        "    loss = criterion(output, real_label)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss"
      ],
      "metadata": {
        "id": "t2nR6aKV4Dc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Specifying the noise\n"
      ],
      "metadata": {
        "id": "gGlS-6PU5Su6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "noise = create_noise(sample_size, nz)"
      ],
      "metadata": {
        "id": "_OKjAvxs5VGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator.train()\n",
        "discriminator.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "918tzSly5Wdr",
        "outputId": "350bcd9c-afb4-41e0-d2b7-33e9748b7658"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discriminator(\n",
              "  (main): Sequential(\n",
              "    (0): Linear(in_features=784, out_features=1024, bias=True)\n",
              "    (1): LeakyReLU(negative_slope=0.2)\n",
              "    (2): Dropout(p=0.3, inplace=False)\n",
              "    (3): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (4): LeakyReLU(negative_slope=0.2)\n",
              "    (5): Dropout(p=0.3, inplace=False)\n",
              "    (6): Linear(in_features=512, out_features=256, bias=True)\n",
              "    (7): LeakyReLU(negative_slope=0.2)\n",
              "    (8): Dropout(p=0.3, inplace=False)\n",
              "    (9): Linear(in_features=256, out_features=1, bias=True)\n",
              "    (10): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Training Loop\n"
      ],
      "metadata": {
        "id": "PyDkdayb5Yxu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    loss_g = 0.0\n",
        "    loss_d = 0.0\n",
        "    for bi, data in enumerate(train_loader):\n",
        "        image, _ = data\n",
        "        image = image.to(device)\n",
        "        b_size = len(image)\n",
        "        # run the discriminator for k number of steps\n",
        "        for step in range(k):\n",
        "            data_fake = generator(create_noise(b_size, nz)).detach()\n",
        "            data_real = image\n",
        "            # train the discriminator network\n",
        "            loss_d += train_discriminator(optim_d, data_real, data_fake)\n",
        "        data_fake = generator(create_noise(b_size, nz))\n",
        "        # train the generator network\n",
        "        loss_g += train_generator(optim_g, data_fake)\n",
        "    # create the final fake image for the epoch\n",
        "    generated_img = generator(noise).cpu().detach()\n",
        "    # make the images as grid\n",
        "    generated_img = make_grid(generated_img)\n",
        "    #save the generated torch tensor models to disk\n",
        "    #save_generator_image(generated_img, f\"../outputs/gen_img{epoch}.png\")\n",
        "    images.append(generated_img)\n",
        "    epoch_loss_g = loss_g / bi # total generator loss for the epoch\n",
        "    epoch_loss_d = loss_d / bi # total discriminator loss for the epoch\n",
        "    losses_g.append(epoch_loss_g)\n",
        "    losses_d.append(epoch_loss_d)\n",
        "\n",
        "    print(f\"Epoch {epoch} of {epochs}\")\n",
        "    print(f\"Generator loss: {epoch_loss_g:.8f}, Discriminator loss: {epoch_loss_d:.8f}\")"
      ],
      "metadata": {
        "id": "_k533u7K5XpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x in range(len(losses_g)):\n",
        "  losses_g[x] = losses_g[x].item()\n",
        "  losses_d[x] =losses_d[x].item()"
      ],
      "metadata": {
        "id": "oxrZrMV2iPaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot and save the generator and discriminator loss\n",
        "plt.figure()\n",
        "plt.plot(losses_g, label='Generator loss')\n",
        "plt.plot(losses_d, label='Discriminator Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Oef29sca5qJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6Txkuwvn1Bob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(images[0].numpy()[0], cmap='gray_r')"
      ],
      "metadata": {
        "id": "IofvK09I1yNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(images[-1].numpy()[0], cmap='gray_r')"
      ],
      "metadata": {
        "id": "yqIO7Rv62qHv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}